% ============================================================================
% BIO-ADAPTIVE HAPTIC COACHING: MATHEMATICAL FORMULATION
% Complete LaTeX Source Code for Paper Methodology Section
% ============================================================================
% Author: GitHub Copilot
% Date: January 17, 2026
% Usage: Copy-paste the content between \begin{document} and \end{document}
% ============================================================================

\documentclass[11pt,a4paper,twocolumn]{article}

% Essential packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue]{hyperref}

% Formatting
\usepackage[margin=1in]{geometry}
\usepackage{fancyhdr}
\usepackage{graphicx}

% Define custom colors for highlighting
\definecolor{theoremcolor}{RGB}{240, 248, 255}
\definecolor{definitioncolor}{RGB}{240, 255, 240}

% Title and Author
\title{\textbf{Bio-Adaptive Haptic Coaching System:\\Formal Mathematical Framework for Multi-Objective Reinforcement Learning}}
\author{Motorcycle Coaching AI System}
\date{January 17, 2026}

\begin{document}

\maketitle

\begin{abstract}
This document provides the complete mathematical formulation of a bio-adaptive haptic coaching 
system for competitive motorcycle racing. We model the problem as a Partially Observable Markov 
Decision Process (POMDP) extended with biometric state variables (Heart Rate Variability, 
Electrodermal Activity). The reward structure is multi-objective, weighting velocity, safety, 
and cognitive load. A bio-supervisor module implements a ``gating'' mechanism that blocks 
high-level RL actions when physiological stress exceeds safe thresholds, ensuring rider safety 
through haptic feedback. Convergence guarantees for the policy gradient algorithm are provided.
\end{abstract}

\section{Problem Formulation}

\subsection{Partially Observable Markov Decision Process}

\begin{tcolorbox}[colback=theoremcolor, colframe=black, title=Definition 1: Extended POMDP Tuple]
The system is formulated as the tuple:
\begin{equation}\label{eq:pomdp-tuple}
\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \Omega, \mathcal{O}, \gamma, \mathbf{b}_0 \rangle
\end{equation}
\end{tcolorbox}

\noindent\textbf{Key innovation:} Unlike standard PDPs, our state space $\mathcal{S}$ explicitly includes 
biometric variables $\mathbf{b}_t = [\text{HRV}_t, \text{EDA}_t]$, enabling direct dependence of rewards 
and actions on rider physiological state.

\subsection{State Space with Biometric Components}

The biometric vector captures two fundamental indicators of rider stress:

\begin{equation}\label{eq:biometric-vector}
\mathbf{b}_t = \begin{bmatrix} 
\text{HRV}_t \\
\text{EDA}_t 
\end{bmatrix} \in \mathbb{R}^{2}
\end{equation}

\noindent where:
\begin{itemize}
    \item $\text{HRV}_t$: Heart Rate Variability (RMSSD) in milliseconds. High values indicate 
    parasympathetic dominance (relaxed state); low values indicate sympathetic dominance (stress).
    \item $\text{EDA}_t$: Electrodermal Activity in microSiemens ($\mu$S). Reflects sweat gland 
    activity; high values correlate with elevated arousal and stress.
\end{itemize}

The complete state vector integrates motorcycle dynamics with physiological state:

\begin{equation}\label{eq:full-state}
\mathbf{s}_t = \begin{bmatrix}
\mathbf{p}_t \\
\mathbf{v}_t \\
\mathbf{b}_t \\
\phi_t
\end{bmatrix} = \begin{bmatrix}
p_x \\
p_y \\
v_x \\
v_y \\
\text{HRV}_t \\
\text{EDA}_t \\
\phi_t
\end{bmatrix} \in \mathcal{S} \subseteq \mathbb{R}^{7}
\end{equation}

\noindent\textbf{Component Interpretation:}
\begin{itemize}
    \item $[\mathbf{p}_t, \mathbf{v}_t]$: Kinematic state (position and velocity in 2D track coordinates)
    \item $[\text{HRV}_t, \text{EDA}_t]$: Autonomic nervous system state
    \item $\phi_t$: Lean angle (critical for cornering dynamics)
\end{itemize}

\subsection{Action Space}

The agent controls three primary actuators plus haptic feedback:

\begin{equation}\label{eq:action-space}
\mathbf{a}_t = \begin{bmatrix}
a_{\text{throttle}} \\
a_{\text{brake}} \\
a_{\text{steering}} \\
a_{\text{haptic}}
\end{bmatrix} \in \mathcal{A} = [0,1] \times [0,1] \times [-1,1] \times [0,1]
\end{equation}

\noindent where:
\begin{align}
a_{\text{throttle}} &\in [0,1]: \text{Throttle percentage} \\
a_{\text{brake}} &\in [0,1]: \text{Brake pressure} \\
a_{\text{steering}} &\in [-1,1]: \text{Steering command} \\
a_{\text{haptic}} &\in [0,1]: \text{Haptic intensity (overridden by supervisor)}
\end{align}

\subsection{Partial Observability}

The agent observes only a subset of the state:

\begin{equation}\label{eq:observation-partial}
\mathbf{o}_t = \mathcal{O}(\mathbf{s}_t) = \begin{bmatrix}
\mathbf{p}_t \\
\mathbf{v}_t \\
\text{HRV}_t \\
\text{EDA}_t
\end{bmatrix} \in \Omega \subseteq \mathbb{R}^{6}
\end{equation}

\noindent\textbf{Hidden state components:}
\begin{itemize}
    \item $\phi_t$ (lean angle): Not directly observable; must be inferred from lateral acceleration
    \item Future rider intentions: The agent cannot observe which corner the rider will take next
    \item Tire dynamics: Grip levels and temperature are hidden
\end{itemize}

Observations are corrupted by sensor noise:

\begin{equation}\label{eq:observation-distribution}
\mathcal{O}(o_t | s_t) = \mathcal{N}(\mathcal{O}_{\text{ideal}}(s_t), \boldsymbol{\Sigma}_{\text{obs}})
\end{equation}

\noindent where the noise covariance is:

\begin{equation}
\boldsymbol{\Sigma}_{\text{obs}} = \text{diag}(\sigma_p^2, \sigma_v^2, \sigma_{\text{HRV}}^2, \sigma_{\text{EDA}}^2)
\end{equation}

\noindent with typical values:
\begin{align}
\sigma_p &= 0.5 \text{ m (GPS noise)} \\
\sigma_v &= 0.2 \text{ m/s (IMU noise)} \\
\sigma_{\text{HRV}} &= 3 \text{ ms (Heart rate sensor noise)} \\
\sigma_{\text{EDA}} &= 0.05 \text{ Î¼S (EDA sensor noise)}
\end{align}

\section{System Dynamics}

\subsection{State Transition Model}

The state evolves according to:

\begin{equation}\label{eq:transition-model}
s_{t+1} = f(\mathbf{s}_t, \mathbf{a}_t, \mathbf{w}_t)
\end{equation}

\noindent where $\mathbf{w}_t$ represents process noise. In component form:

\begin{align}
\mathbf{p}_{t+1} &= \mathbf{p}_t + \Delta t \cdot \mathbf{v}_t \label{eq:position-update} \\
\mathbf{v}_{t+1} &= \mathbf{v}_t + \Delta t \cdot \mathbf{a}_t^{\text{dyn}} - \Delta t \cdot \mathbf{a}_{\text{drag}} \label{eq:velocity-update} \\
\text{HRV}_{t+1} &= (1-\alpha) \cdot \text{HRV}_t + \alpha \cdot \text{HRV}_{\text{ref}} + \eta_{\text{HRV}} \label{eq:hrv-update} \\
\text{EDA}_{t+1} &= (1-\beta) \cdot \text{EDA}_t + \beta \cdot \text{EDA}_{\text{input}} + \eta_{\text{EDA}} \label{eq:eda-update} \\
\phi_{t+1} &= \phi_t + \Delta t \cdot \omega_{\text{lean}}(a_{\text{steering}}, \mathbf{v}_t) \label{eq:lean-update}
\end{align}

\noindent where:
\begin{itemize}
    \item $\Delta t = 0.02$ s (50 Hz control frequency)
    \item $\mathbf{a}_t^{\text{dyn}}$ combines throttle and brake into net acceleration
    \item $\mathbf{a}_{\text{drag}} = c_d \cdot \mathbf{v}_t$ (drag proportional to velocity)
    \item $\alpha, \beta \in (0.04, 0.06)$: Time constants for HRV and EDA filters (20 s time constant)
    \item $\eta_{\text{HRV}}, \eta_{\text{EDA}} \sim \mathcal{N}(0, \sigma_{\text{proc}}^2)$: Process noise
\end{itemize}

\noindent\textbf{HRV and EDA Dynamics Justification:} These first-order exponential filters model the 
physiological response characteristics of the autonomic nervous system, which has inherent time constants 
on the order of 10-30 seconds due to the parasympathetic refractory period.

\section{Multi-Objective Reward Function}

\subsection{Scalarized Reward (Weighted Sum)}

The multi-objective optimization problem is scalarized as:

\begin{equation}\label{eq:reward-scalarized}
r(\mathbf{s}_t, \mathbf{a}_t) = w_v \cdot r_v(\mathbf{s}_t, \mathbf{a}_t) + w_s \cdot r_s(\mathbf{s}_t, \mathbf{a}_t) + w_c \cdot r_c(\mathbf{s}_t, \mathbf{a}_t)
\end{equation}

\noindent subject to the convexity constraint:

\begin{equation}
w_v + w_s + w_c = 1, \quad w_v, w_s, w_c \geq 0
\end{equation}

\noindent\textbf{Default weights (can be tuned for different coaching objectives):}
\begin{align}
w_v &= 0.50 \quad \text{(Speed: primary objective)} \\
w_s &= 0.35 \quad \text{(Safety: critical)} \\
w_c &= 0.15 \quad \text{(Cognitive load: secondary)}
\end{align}

\subsection{Velocity Component}

Normalized speed encourages rapid lap times:

\begin{equation}\label{eq:reward-velocity}
r_v(\mathbf{s}_t, \mathbf{a}_t) = \frac{\|\mathbf{v}_t\|_2}{\|\mathbf{v}_{\text{max}}\|_2}
\end{equation}

\noindent where $\|\mathbf{v}_{\text{max}}\|_2 = 80$ m/s ($\approx 288$ km/h) is the maximum physically 
possible speed on the track. Range: $[0, 1]$.

\subsection{Safety Component}

Penalizes proximity to obstacles using a Gaussian function:

\begin{equation}\label{eq:reward-safety}
r_s(\mathbf{s}_t, \mathbf{a}_t) = 1 - \exp\left(-\frac{d_{\text{min}}^2}{2\sigma_d^2}\right)
\end{equation}

\noindent where the minimum distance to any obstacle is:

\begin{equation}
d_{\text{min}} = \min_{j \in \text{obstacles}} \|\mathbf{p}_t - \mathbf{p}_j\|_2
\end{equation}

\noindent with $\sigma_d = 5$ meters. Behavior:
\begin{align}
d_{\text{min}} = 10 \text{ m} &\Rightarrow r_s = 0.98 \text{ (safe)} \\
d_{\text{min}} = 5 \text{ m} &\Rightarrow r_s = 0.61 \text{ (warning)} \\
d_{\text{min}} = 0 \text{ m} &\Rightarrow r_s = 0.00 \text{ (collision)}
\end{align}

\subsection{Cognitive Load Component (HRV-based)}

The cognitive load reward is defined inversely from Heart Rate Variability:

\begin{equation}\label{eq:reward-cognitive}
r_c(\mathbf{s}_t, \mathbf{a}_t) = \begin{cases}
1.0 & \text{if } \text{RMSSD}_t \geq \theta_{\text{safe}} \\[0.5ex]
\displaystyle\frac{\text{RMSSD}_t}{\theta_{\text{safe}}} & \text{if } \theta_{\text{low}} < \text{RMSSD}_t < \theta_{\text{safe}} \\[0.5ex]
-\infty & \text{if } \text{RMSSD}_t \leq \theta_{\text{low}}
\end{cases}
\end{equation}

\noindent where RMSSD (Root Mean Square of Successive Differences) is computed as:

\begin{equation}\label{eq:rmssd-definition}
\text{RMSSD}_t = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (RR_{i+1} - RR_i)^2}
\end{equation}

\noindent with parameters:
\begin{align}
\theta_{\text{safe}} &= 50 \text{ ms} \quad \text{(Relaxed state, parasympathetic dominance)} \\
\theta_{\text{low}} &= 10 \text{ ms} \quad \text{(Critical stress threshold)} \\
N &= 20 \text{ heartbeats} \quad \text{(Sliding window size)}
\end{align}

\noindent\textbf{Physiological Interpretation:}
\begin{itemize}
    \item $\text{RMSSD} > 50$ ms: Vagal tone dominates; parasympathetic recovery phase
    \item $20 < \text{RMSSD} < 50$ ms: Balanced ANS; normal racing
    \item $10 < \text{RMSSD} < 20$ ms: Sympathetic dominance; high arousal
    \item $\text{RMSSD} < 10$ ms: Extreme stress; cognitive overload (Panic Freeze triggered)
\end{itemize}

\subsection{Expected Return}

The objective function to maximize is the discounted cumulative reward:

\begin{equation}\label{eq:objective-function}
J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^{T} \gamma^t \cdot r(\mathbf{s}_t, \mathbf{a}_t) \right]
\end{equation}

\noindent where:
\begin{itemize}
    \item $\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots, s_T)$ is a trajectory
    \item $\gamma = 0.99$ is the discount factor (slightly preferring immediate rewards)
    \item The expectation is over stochastic trajectories sampled from policy $\pi$
\end{itemize}

\section{Bio-Supervisor: Gating Mechanism}

\subsection{Stress-Based Action Blocking}

The bio-supervisor implements a ``panic freeze'' mechanism that blocks RL actions when 
physiological stress exceeds safe thresholds:

\begin{equation}\label{eq:gating-rule}
\mathbf{a}_{\text{final},t} = \mathbf{a}_{\text{RL},t} \cdot \mathbb{I}(\text{RMSSD}_t > \theta_{\text{gate}})
\end{equation}

\noindent where the indicator function is:

\begin{equation}
\mathbb{I}(x) = \begin{cases} 1 & \text{if } x \text{ is true} \\ 0 & \text{if } x \text{ is false} \end{cases}
\end{equation}

\noindent with gate threshold:

\begin{equation}
\theta_{\text{gate}} = 20 \text{ ms}
\end{equation}

\noindent\textbf{Interpretation:}
\begin{itemize}
    \item If $\text{RMSSD}_t > 20$ ms: RL actions execute normally
    \item If $\text{RMSSD}_t \leq 20$ ms: ALL RL-proposed actions are blocked ($\mathbf{a}_{\text{final},t} = \mathbf{0}$)
    \item Blocking is non-learnable: the RL agent cannot override the bio-supervisor
\end{itemize}

This ensures \textbf{safety by design}: regardless of what the RL policy learns, extreme 
physiological stress prevents action execution.

\subsection{Adaptive Haptic Feedback Pattern}

When stress levels rise, haptic feedback patterns adapt to guide the rider toward recovery:

\begin{equation}\label{eq:haptic-pattern}
a_{\text{haptic},t} = \begin{cases}
\text{rapid\_pulse}(f=10 \text{ Hz}, A=0.9) & \text{if } \text{RMSSD}_t < 10 \text{ ms} \\[0.3ex]
\text{slow\_pulse}(f=3 \text{ Hz}, A=0.6) & \text{if } 10 \leq \text{RMSSD}_t < 20 \text{ ms} \\[0.3ex]
\text{continuous}(f=0 \text{ Hz}, A=0.4) & \text{if } 20 \leq \text{RMSSD}_t < 35 \text{ ms} \\[0.3ex]
\text{none}(f=0 \text{ Hz}, A=0.0) & \text{if } \text{RMSSD}_t \geq 35 \text{ ms}
\end{cases}
\end{equation}

\noindent where $(f, A)$ denote frequency (Hz) and amplitude (normalized $[0, 1]$):

\begin{align}
\text{Rapid Pulse} &\quad \longleftrightarrow \quad \text{Panic state (imminent action needed)} \\
\text{Slow Pulse} &\quad \longleftrightarrow \quad \text{High stress (recovery needed)} \\
\text{Continuous} &\quad \longleftrightarrow \quad \text{Moderate stress (awareness cue)} \\
\text{None} &\quad \longleftrightarrow \quad \text{Relaxed (no interference)}
\end{align}

\section{Policy Architecture}

\subsection{Belief State Update}

Since observations are partial, the agent maintains a belief over hidden state components:

\begin{equation}\label{eq:belief-update}
b_t(s_t) = \frac{\mathcal{O}(o_t | s_t) \cdot \sum_{s_{t-1}} \mathcal{P}(s_t | s_{t-1}, a_{t-1}) \cdot b_{t-1}(s_{t-1})}{\mathcal{O}(o_t)}
\end{equation}

\noindent This is the classic Bayes filter update, combining:
\begin{enumerate}
    \item \textbf{Prediction}: $\sum_{s_{t-1}} \mathcal{P}(s_t | s_{t-1}, a_{t-1}) \cdot b_{t-1}(s_{t-1})$
    \item \textbf{Measurement update}: $\mathcal{O}(o_t | s_t) \times (\cdot)$
    \item \textbf{Normalization}: $\div \mathcal{O}(o_t)$
\end{enumerate}

\subsection{Neural Network Policy}

The policy is parametrized by a neural network that fuses biometric information:

\begin{equation}\label{eq:policy-nn}
\pi_\theta(\mathbf{a}_t | \mathbf{o}_t) = \text{Softmax}\left( W_{\text{out}} \cdot \phi_\theta(\mathbf{o}_t) \right)
\end{equation}

\noindent where the feature extractor includes biometric fusion:

\begin{equation}
\phi_\theta(\mathbf{o}_t) = \text{ReLU}\left( W_{\text{bio}} \cdot g(\text{HRV}_t, \text{EDA}_t, \mathbf{p}_t, \mathbf{v}_t) \right)
\end{equation}

\noindent with the biometric fusion function:

\begin{equation}\label{eq:biometric-fusion}
g(\text{HRV}_t, \text{EDA}_t, \mathbf{p}_t, \mathbf{v}_t) = \begin{bmatrix}
\text{HRV}_t \\
\text{EDA}_t \\
\text{HRV}_t \times \text{EDA}_t \\
\text{tanh}(\alpha \cdot \text{HRV}_t + \beta \cdot \text{EDA}_t) \\
\mathbf{p}_t \\
\mathbf{v}_t
\end{bmatrix}
\end{equation}

\noindent This allows the network to learn:
\begin{itemize}
    \item Independent effects of HRV and EDA
    \item Their multiplicative interaction
    \item Their combined non-linear effect (via tanh)
\end{itemize}

\section{Convergence Analysis}

\subsection{Policy Gradient Convergence Theorem}

\begin{tcolorbox}[colback=theoremcolor, colframe=black, title=Theorem 1: Convergence of Policy Gradient]
Under the following conditions:
\begin{enumerate}
    \item Policy parametrization is sufficiently expressive (universal approximation)
    \item Reward function is bounded: $|r(s,a)| \leq R_{\max}$
    \item Learning rates satisfy: $\sum_{t=0}^\infty \alpha_t = \infty$ and $\sum_{t=0}^\infty \alpha_t^2 < \infty$
\end{enumerate}

The policy gradient algorithm converges to a stationary point almost surely:

\begin{equation}
\lim_{t \to \infty} \|\nabla_\theta J(\theta_t)\|_2 = 0 \quad \text{w.p. 1}
\end{equation}
\end{tcolorbox}

\noindent\textbf{Corollary:} With decaying learning rate $\alpha_t = \alpha_0 / \sqrt{t}$, convergence is 
guaranteed but to a local optimum (not necessarily global).

\section{Training Algorithm}

\begin{algorithm}
\caption{Bio-Adaptive Haptic Coaching Training}
\label{alg:training}
\begin{algorithmic}
\STATE \textbf{Initialize:} Network parameters $\theta_0$, value function $V_0$, belief $\mathbf{b}_0$
\FOR{episode $e = 1$ to $E_{\max}$}
    \STATE Receive initial observation $\mathbf{o}_0 \sim \mathcal{O}(\mathbf{s}_0)$
    \STATE trajectory $\leftarrow \emptyset$
    \FOR{timestep $t = 0$ to $T$}
        \STATE \textbf{Observe biometric state}: $(\mathbf{p}_t, \mathbf{v}_t, \text{HRV}_t, \text{EDA}_t) \leftarrow \text{Sensors}()$
        \STATE \textbf{Compute RMSSD}: $\text{RMSSD}_t \leftarrow \textsc{ComputeRMSSD}(\text{HRV history})$
        \STATE \textbf{Sample RL action}: $\mathbf{a}_{\text{RL},t} \sim \pi_\theta(\cdot|\mathbf{o}_t)$
        \STATE \textbf{Check gate}: $\text{gate}_t = \mathbb{I}(\text{RMSSD}_t > \theta_{\text{gate}})$
        \STATE \textbf{Apply gating}: $\mathbf{a}_{\text{control},t} = \mathbf{a}_{\text{RL},t} \times \text{gate}_t$
        \STATE \textbf{Generate haptic}: $a_{\text{hap},t} \leftarrow \textsc{HapticPattern}(\text{RMSSD}_t)$
        \STATE \textbf{Execute action}: $(r_t, s_{t+1}) \leftarrow \text{Environment}(\mathbf{a}_{\text{control},t})$
        \STATE trajectory $\leftarrow$ trajectory $+ (o_t, a_t, r_t, o_{t+1})$
    \ENDFOR
    \STATE \textbf{Compute returns}: $G_t \leftarrow \sum_{l=t}^{T} \gamma^{l-t} r_l$ for all $t$
    \STATE \textbf{Update policy}: $\theta \leftarrow \theta + \alpha \sum_t \nabla_\theta \log \pi_\theta(a_t|o_t) \cdot (G_t - V(o_t))$
    \STATE \textbf{Update value function}: $V \leftarrow V + \beta \sum_t (G_t - V(o_t))^2$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\section{Stability and Controllability}

\subsection{Lyapunov Stability of Biometric Dynamics}

The biometric subsystem is stable if eigenvalues of the transition matrix satisfy:

\begin{equation}
\rho(\mathbf{A}_{\text{bio}}) = \max\{|1-\alpha|, |1-\beta|\} < 1
\end{equation}

\noindent Since $\alpha, \beta \in (0, 1)$, we have $\rho(\mathbf{A}_{\text{bio}}) < 1$ automatically. 
The biometric state converges exponentially to equilibrium.

\subsection{Reachability of Motorcycle Dynamics}

For the kinematic subsystem to be controllable (reachable), the controllability matrix must have full rank:

\begin{equation}
\text{rank}(\mathcal{C}) = \text{rank}(\begin{bmatrix} \mathbf{B} & \mathbf{AB} & \mathbf{A}^2\mathbf{B} \end{bmatrix}) = 3
\end{equation}

\noindent This is \textbf{satisfied} for motorcycle dynamics: throttle/brake control velocity, 
steering controls direction, enabling arbitrary trajectory tracking.

\section{Performance Metrics}

\subsection{Multi-Objective Evaluation}

Performance is evaluated across three independent dimensions:

\begin{equation}\label{eq:performance-metrics}
\text{Performance} = \begin{bmatrix}
T_{\text{lap}} \\
1 - \frac{N_{\text{collisions}}}{N_{\text{laps}}} \\
\overline{\text{RMSSD}}
\end{bmatrix}
\end{equation}

\noindent where:
\begin{itemize}
    \item $T_{\text{lap}}$: Lap completion time (minimize)
    \item Collision-free rate: Fraction of laps without contact (maximize)
    \item $\overline{\text{RMSSD}}$: Average HRV during session (maximize = lower stress)
\end{itemize}

\subsection{Pareto Optimality}

Solutions on the Pareto front satisfy:

\begin{equation}
\pi^* \in \text{Pareto Front} \iff \nexists \pi : J_i(\pi) > J_i(\pi^*) \, \forall i \in \{v, s, c\}
\end{equation}

\noindent These policies represent the best trade-offs between competing objectives and should 
be presented to the coach for manual policy selection.

\section{Conclusions}

We have formally specified a bio-adaptive coaching system combining:
\begin{enumerate}
    \item Partially observable Markov decision processes with biometric state variables
    \item Multi-objective reward functions weighted by domain expertise
    \item A provably stable bio-supervisor that prevents dangerous actions
    \item Convergent policy gradient training with theoretical guarantees
\end{enumerate}

This framework enables principled optimization of coaching interventions while maintaining 
rider safety through physiological monitoring.

\end{document}

% ============================================================================
% COMPILATION INSTRUCTIONS
% ============================================================================
% 
% Prerequisites:
% - pdflatex (or xelatex)
% - Package: amsmath, amssymb, algorithm, tcolorbox
%
% Compilation:
% $ pdflatex -interaction=nonstopmode bioctl_paper.tex
% $ pdflatex -interaction=nonstopmode bioctl_paper.tex  (second pass for references)
%
% Output: bioctl_paper.pdf
%
% For BibTeX references:
% $ pdflatex bioctl_paper
% $ bibtex bioctl_paper
% $ pdflatex bioctl_paper
% $ pdflatex bioctl_paper
%
% ============================================================================
