# Moto-Edge-RL Training & Deployment Pipeline

Complete MLOps pipeline for training and deploying real-time haptic coaching agents for competitive motorcycle racing.

## ðŸ“‹ Overview

This pipeline orchestrates the entire machine learning workflow:
- **Data Generation**: Synthetic motorcycle racing telemetry (Minari format)
- **Training**: Hybrid offline-online RL (Behavior Cloning + PPO)
- **Deployment**: ONNX â†’ TensorFlow â†’ TFLite with int8 quantization

### Technology Stack

- **Environment**: Gymnasium (custom motorcycle physics)
- **Multi-Agent Orchestration**: PettingZoo (Agent 1: Trajectory, Agent 2: Coach)
- **Offline RL Dataset**: Minari (standard format for offline RL)
- **Algorithm**: PPO (Proximal Policy Optimization) via Stable-Baselines3
- **Edge Export**: PyTorch â†’ ONNX â†’ TensorFlow â†’ TFLite
- **Target Device**: ESP32 microcontroller (int8 quantization)

---

## ðŸš€ Quick Start

### Option 1: Run Complete Pipeline (Recommended)

```bash
# Run all steps in order
./run_pipeline.sh

# Run with custom configuration
./run_pipeline.sh --laps 100 --timesteps 100000 --eval-episodes 10
```

**Total runtime**: ~10 hours
- Data generation: 30 minutes
- Training: 6 hours
- Model export: 30 minutes

### Option 2: Run Individual Steps

#### Step 1: Generate Synthetic Data

```bash
python3 src/data/generate_synthetic_data.py \
    --laps 100 \
    --output-dir data/processed \
    --seed 42
```

**Output**: 
- `pro_rider_dataset.hdf5` - Professional rider trajectories
- `amateur_rider_dataset.hdf5` - Amateur rider trajectories

#### Step 2: Train Hybrid Model

```bash
python3 src/training/train_hybrid.py \
    --pro-dataset data/processed/pro_rider_dataset.hdf5 \
    --amateur-dataset data/processed/amateur_rider_dataset.hdf5 \
    --output-model models/moto_edge_policy.zip \
    --timesteps 100000 \
    --eval-episodes 10 \
    --seed 42
```

**Output**:
- `moto_edge_policy.zip` - Trained PPO model (Stable-Baselines3 format)
- `model_metadata.json` - Training metadata and evaluation results

#### Step 3: Export to Edge Formats

```bash
python3 src/deployment/export_to_edge.py \
    --model models/moto_edge_policy.zip \
    --output-dir models/edge_deployment/
```

**Output**:
- `moto_edge_policy.onnx` - ONNX model (cross-platform)
- `moto_edge_policy_tf/` - TensorFlow SavedModel
- `moto_edge_policy.tflite` - TFLite for edge devices
- `moto_edge_policy_quantized.tflite` - Optimized (int8 quantization)

---

## ðŸ“¦ Installation

### Requirements

- Python 3.8+
- 2GB free disk space
- ~10 hours for complete pipeline

### Setup

```bash
# 1. Clone repository
git clone https://github.com/rubences/Coaching-for-Competitive-Motorcycle-Racing.git
cd Coaching-for-Competitive-Motorcycle-Racing

# 2. Create virtual environment (recommended)
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Verify installation
python3 -c "import gymnasium, stable_baselines3, minari, torch; print('âœ“ All dependencies installed')"
```

---

## ðŸ“Š Pipeline Architecture

### 1. Data Generation (`src/data/generate_synthetic_data.py`)

**Rider Models**:
- **Pro Rider**: Smooth inputs, optimal apex speed, minimal line deviation
- **Amateur Rider**: Jerky inputs, suboptimal speed, variable line choice

**Dataset Format**: Minari (HDF5)
```
â”œâ”€â”€ metadata
â”‚   â”œâ”€â”€ dataset_name
â”‚   â”œâ”€â”€ rider_profile
â”‚   â”œâ”€â”€ creation_date
â”‚   â””â”€â”€ total_episodes
â””â”€â”€ episodes
    â”œâ”€â”€ episode_0
    â”‚   â”œâ”€â”€ observations (shape: [steps, 8])
    â”‚   â”œâ”€â”€ actions (shape: [steps, 3])
    â”‚   â”œâ”€â”€ rewards (shape: [steps])
    â”‚   â”œâ”€â”€ truncations (shape: [steps])
    â”‚   â””â”€â”€ terminations (shape: [steps])
    â””â”€â”€ ...
```

**Observation Space** (8D):
- `velocity` [0, 100] m/s
- `roll_angle` [-60, 60] degrees
- `lateral_g` [-2.0, 2.0] G-forces
- `distance_to_apex` [0, 500] m
- `throttle_position` [0, 1]
- `brake_pressure` [0, 1]
- `racing_line_deviation` [-10, 10] m
- `tire_friction_usage` [0, 1]

**Action Space** (3D - Haptic Feedback):
- `haptic_left_intensity` [0, 1]
- `haptic_right_intensity` [0, 1]
- `haptic_frequency` [50, 300] Hz

### 2. Hybrid Training (`src/training/train_hybrid.py`)

#### Step 2a: Offline Pre-training (Behavior Cloning)

**Goal**: Learn racing basics from expert demonstrations

```
Loss = MSE(Ï€_Î¸(s), a_expert)
Dataset: 70% Pro + 30% Amateur trajectories
```

**Benefits**:
- Accelerated convergence
- Stable initial policy
- Foundation for fine-tuning

#### Step 2b: Online Fine-tuning (PPO)

**Configuration**:
- Algorithm: Proximal Policy Optimization
- Total timesteps: 100,000
- Gamma (discount): 0.99
- GAE Î»: 0.95
- Learning rate: 3e-4

**Reward Function**:
```
R(t) = -lap_time_penalty + safety_bonus - action_regularization

Where:
- lap_time_penalty: Encourages faster completion
- safety_bonus: +0.1 if lateral_g < 1.8 AND tire_friction < 0.95
- action_regularization: Penalizes extreme control inputs
```

**Hyperparameters**:
```python
{
    "policy": "MlpPolicy",           # Multi-layer perceptron
    "learning_rate": 3e-4,           # Adaptive scheduling available
    "n_steps": 2048,                 # Rollout length
    "batch_size": 64,                # Mini-batch size
    "n_epochs": 10,                  # PPO epochs per update
    "gamma": 0.99,                   # Discount factor
    "gae_lambda": 0.95,              # GAE parameter
    "clip_range": 0.2,               # PPO clip coefficient
    "normalize_advantage": true,
    "ent_coef": 0.0,                 # Entropy regularization
    "vf_coef": 0.5                   # Value function weight
}
```

#### Step 2c: Evaluation

**Metrics**:
- Average lap time (seconds)
- Standard deviation of lap times
- Minimum/Maximum lap times
- Average episodic reward
- Success rate (% completed without crashes)
- Total safety violations
- Violations per episode

### 3. Model Export (`src/deployment/export_to_edge.py`)

**Conversion Pipeline**:

```
PyTorch (SB3)
    â†“
    ONNX
    â”œâ”€â†’ OpenVINO (Intel edge inference)
    â”œâ”€â†’ CoreML (Apple devices)
    â””â”€â†’ TensorFlow
        â†“
        TFLite (mobile/embedded)
        â”œâ”€â†’ Dynamic range quantization
        â”œâ”€â†’ Full integer quantization
        â””â”€â†’ Float16 quantization
```

**ESP32 Target Specifications**:
- CPU: Xtensa LX6 dual-core @ 240 MHz
- RAM: 520 KB SRAM (shared with system)
- Flash: 4 MB (shared with firmware)
- Required inference latency: <50ms
- Model size target: <2 MB

**Quantization Strategy: Dynamic Range (Post-Training)**

Reduces model size by ~4x with minimal accuracy loss:
```
Weights: float32 â†’ int8 (per-tensor dynamic range)
Activations: float32 (optional per-tensor quantization)
Bias: Not quantized
Recovery: Asymmetric quantization ensures compatibility
```

---

## ðŸ“ Directory Structure

```
project/
â”œâ”€â”€ run_pipeline.sh                    # Main orchestration script
â”œâ”€â”€ examples.py                        # Quick start examples
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ generate_synthetic_data.py # Data generation
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â””â”€â”€ train_hybrid.py            # Training pipeline
â”‚   â”œâ”€â”€ deployment/
â”‚   â”‚   â””â”€â”€ export_to_edge.py          # Model export
â”‚   â””â”€â”€ moto_edge_rl/                  # Main package
â”œâ”€â”€ data/
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ pro_rider_dataset.hdf5
â”‚       â””â”€â”€ amateur_rider_dataset.hdf5
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ moto_edge_policy.zip           # Trained model
â”‚   â”œâ”€â”€ model_metadata.json
â”‚   â”œâ”€â”€ checkpoints/                   # Training checkpoints
â”‚   â””â”€â”€ edge_deployment/
â”‚       â”œâ”€â”€ moto_edge_policy.onnx
â”‚       â”œâ”€â”€ moto_edge_policy_tf/
â”‚       â”œâ”€â”€ moto_edge_policy.tflite
â”‚       â””â”€â”€ moto_edge_policy_quantized.tflite
â”œâ”€â”€ simulation/
â”‚   â””â”€â”€ motorcycle_env.py              # Environment definition
â””â”€â”€ logs/
    â””â”€â”€ pipeline_*.log                 # Pipeline execution logs
```

---

## ðŸ”§ Advanced Usage

### Custom Configuration

```bash
# Generate more data
./run_pipeline.sh --laps 200

# Longer training
./run_pipeline.sh --timesteps 200000

# Skip data generation (use existing)
./run_pipeline.sh --skip-data

# Skip training (use existing model)
./run_pipeline.sh --skip-train

# Skip export
./run_pipeline.sh --skip-export

# Combined
./run_pipeline.sh --skip-data --laps 50 --timesteps 50000
```

### Programmatic Usage

```python
from src.data.generate_synthetic_data import main as generate_data
from src.training.train_hybrid import main as train_model
from src.deployment.export_to_edge import main as export_model

# Generate data
generate_data(num_laps_per_rider=100, output_dir='data/processed')

# Train
train_model(
    pro_dataset_path='data/processed/pro_rider_dataset.hdf5',
    amateur_dataset_path='data/processed/amateur_rider_dataset.hdf5',
    output_model_path='models/moto_edge_policy.zip',
    total_timesteps=100000
)

# Export
export_model(
    model_path='models/moto_edge_policy.zip',
    output_dir='models/edge_deployment/',
    quantize=True,
    validate=True
)
```

### TensorBoard Monitoring

```bash
# During training
tensorboard --logdir=./logs/
# View at http://localhost:6006
```

### Model Inspection

```python
from stable_baselines3 import PPO

model = PPO.load('models/moto_edge_policy')
print(model.policy)  # View network architecture
print(model.get_parameters())  # Access weights
```

---

## ðŸ“ˆ Expected Performance

### Training Progress (Example)

```
Episode 1: Avg Reward: -2.543
Episode 100: Avg Reward: -1.892
Episode 1000: Avg Reward: -1.234
Episode 10000: Avg Reward: -0.567
```

### Evaluation Metrics (Example)

```
Evaluation Results (10 episodes):
â”œâ”€â”€ Average Lap Time: 45.32 Â± 2.15 seconds
â”œâ”€â”€ Min/Max Lap Time: 42.10 / 49.87 seconds
â”œâ”€â”€ Average Reward: -0.4521
â”œâ”€â”€ Success Rate: 90.0% (9/10 laps completed)
â”œâ”€â”€ Total Safety Violations: 3
â””â”€â”€ Violations per Episode: 0.30
```

### Model Sizes

```
Original Model (SB3):      ~2.8 MB
ONNX:                      ~2.6 MB
TensorFlow SavedModel:     ~3.2 MB
TFLite:                    ~2.4 MB
TFLite Quantized (int8):   ~0.6 MB (4x reduction!)
```

---

## ðŸ› Troubleshooting

### Common Issues

#### 1. "Module not found" errors

```bash
# Ensure you're in the project root
cd /path/to/Coaching-for-Competitive-Motorcycle-Racing

# Verify Python path
python3 -c "import sys; print(sys.path)"

# Run with explicit path
PYTHONPATH=. python3 src/data/generate_synthetic_data.py
```

#### 2. Out of memory (OOM)

```bash
# Reduce laps or batch size
./run_pipeline.sh --laps 50 --timesteps 50000

# Monitor memory
watch -n 1 free -h
```

#### 3. ONNX/TensorFlow conversion fails

```bash
# Verify installation
python3 -c "import torch, onnx, tensorflow; print('OK')"

# Try alternative conversion
python3 src/deployment/export_to_edge.py --model models/moto_edge_policy.zip
```

#### 4. TFLite quantization doesn't reduce size

This is expected with dynamic range quantization. For better reduction:
- Use full integer quantization (requires representative dataset)
- Set `--quantize-full-integer` flag (not yet implemented)

---

## ðŸ“š References

### Papers & Resources

- **PPO**: [Schulman et al., 2017 - Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
- **Behavior Cloning**: [Pomerleau, 1989 - ALVINN: An Autonomous Land Vehicle in a Neural Network](https://www.ri.cmu.edu/pub_files/pub1/pomerleau_dean_1989_1/pomerleau_dean_1989_1.pdf)
- **Minari Dataset**: [Farama Foundation - Minari Documentation](https://minari.readthedocs.io/)
- **TFLite Quantization**: [TensorFlow Lite Quantization Guide](https://www.tensorflow.org/lite/performance/quantization_spec)

### Dependencies

- [Gymnasium](https://gymnasium.farama.org/)
- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/)
- [Minari](https://minari.readthedocs.io/)
- [ONNX](https://onnx.ai/)
- [TensorFlow Lite](https://www.tensorflow.org/lite)

---

## ðŸ“ License

This project is licensed under the MIT License. See LICENSE file for details.

---

## ðŸ¤ Contributing

Contributions are welcome! Please follow these guidelines:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## âš ï¸ Important Notes

### Data Generation

The synthetic data generation is **deterministic** with fixed seed (42). To generate different data:
```bash
python3 src/data/generate_synthetic_data.py --seed 123
```

### Training Reproducibility

For reproducible results:
```bash
./run_pipeline.sh --seed 42
```

### Edge Deployment

The quantized TFLite model (`moto_edge_policy_quantized.tflite`) is ready for:
1. Arduino/ESP32 deployment using TFLite C++ library
2. Mobile app deployment (iOS/Android)
3. Cloud edge inference (AWS Greengrass, Azure Edge)

### Real-Time Constraints

- **Inference latency**: Target <50ms for ESP32
- **Update frequency**: 20 Hz haptic feedback
- **Model latency**: Quantized model typically <10ms on ESP32

---

## ðŸ“§ Support

For issues, questions, or feature requests:
1. Check existing GitHub issues
2. Create new issue with detailed description
3. Include logs from `logs/pipeline_*.log`

---

**Last Updated**: January 2026  
**Pipeline Version**: 1.0.0  
**Status**: Production-Ready âœ…
