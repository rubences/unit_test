{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddb2a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RESUMEN FINAL Y RECOMENDACIONES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚úÖ WORKFLOW COMPLETADO:\\n\")\n",
    "\n",
    "workflow_status = [\n",
    "    (\"‚úÖ Estructura validada\", True),\n",
    "    (\"‚úÖ Configuraci√≥n cargada\", True),\n",
    "    (\"‚úÖ Datos generados\", 'data_generation' in metrics_log['phases']),\n",
    "    (\"‚úÖ Modelo entrenado\", 'training' in metrics_log['phases']),\n",
    "    (\"‚úÖ Evaluaci√≥n completada\", 'evaluation' in metrics_log['phases']),\n",
    "    (\"‚úÖ Visualizaciones generadas\", 'visualization' in metrics_log['phases']),\n",
    "    (\"‚úÖ M√©tricas persistidas\", 'metrics' in str(METRICS_DIR.glob('*'))),\n",
    "]\n",
    "\n",
    "for task, done in workflow_status:\n",
    "    symbol = task[:2]  # Usar el s√≠mbolo del comienzo\n",
    "    print(f\"  {symbol} {task[2:]}\")\n",
    "\n",
    "print(\"\\nüìä ARCHIVOS GENERADOS:\\n\")\n",
    "print(f\"  ‚Ä¢ M√©tricas: {METRICS_DIR}\")\n",
    "print(f\"  ‚Ä¢ Modelos: {MODELS_DIR}\")\n",
    "print(f\"  ‚Ä¢ Logs: {LOGS_DIR}\")\n",
    "print(f\"  ‚Ä¢ Datos: {DATA_DIR}\")\n",
    "\n",
    "print(\"\\nüéØ PR√ìXIMOS PASOS:\\n\")\n",
    "print(\"  1. Revisar m√©tricas en: \" + str(METRICS_DIR))\n",
    "print(\"  2. Analizar modelos en: \" + str(MODELS_DIR))\n",
    "print(\"  3. Ejecutar visualizaciones nuevamente si es necesario\")\n",
    "print(\"  4. Para deployment completo, ejecutar:\")\n",
    "print(\"     python ../scripts/deploy_complete.py\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìç EJECUCI√ìN COMPLETADA\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bc32a0",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Resumen Final y Recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21daa665",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"INVENTARIO DE ARTIFACTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def count_files(directory, pattern='*'):\n",
    "    \"\"\"Contar archivos en directorio\"\"\"\n",
    "    if not directory.exists():\n",
    "        return 0\n",
    "    return len(list(directory.glob(pattern)))\n",
    "\n",
    "artifact_info = {\n",
    "    'Models': (MODELS_DIR, '*.zip'),\n",
    "    'Data': (DATA_DIR, '*.csv'),\n",
    "    'Metrics': (METRICS_DIR, '*.json'),\n",
    "    'Logs': (LOGS_DIR, '*.log'),\n",
    "    'Visualizations': (LOGS_DIR, '*.png'),\n",
    "}\n",
    "\n",
    "print(\"\\nüì¶ ARTIFACTS GENERADOS:\\n\")\n",
    "\n",
    "total_artifacts = 0\n",
    "for category, (path, pattern) in artifact_info.items():\n",
    "    count = count_files(path, pattern)\n",
    "    total_artifacts += count\n",
    "    symbol = \"‚úÖ\" if count > 0 else \"‚è≥\"\n",
    "    print(f\"{symbol} {category:20s}: {count:3d} archivo(s)\")\n",
    "\n",
    "print(f\"\\nüìä Total de artifacts: {total_artifacts}\")\n",
    "\n",
    "# Mostrar contenido de directorios\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONTENIDO DE DIRECTORIOS CLAVE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for dir_name, dir_path in [('Models', MODELS_DIR), ('Logs', LOGS_DIR), ('Data', DATA_DIR)]:\n",
    "    if dir_path.exists():\n",
    "        files = list(dir_path.glob('*'))[:5]  # Primeros 5\n",
    "        print(f\"\\nüìÅ {dir_name}:\")\n",
    "        for f in files:\n",
    "            size_mb = f.stat().st_size / (1024*1024) if f.is_file() else 0\n",
    "            print(f\"   ‚îî‚îÄ {f.name} ({size_mb:.2f} MB)\")\n",
    "        \n",
    "        if len(list(dir_path.glob('*'))) > 5:\n",
    "            remaining = len(list(dir_path.glob('*'))) - 5\n",
    "            print(f\"   ‚îî‚îÄ ... y {remaining} archivo(s) m√°s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46698653",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Inventario de Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d2eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HISTORIAL DE EJECUCIONES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Buscar archivos de m√©tricas\n",
    "metrics_files = sorted(glob.glob(str(METRICS_DIR / 'metrics_*.json')))\n",
    "\n",
    "if metrics_files:\n",
    "    print(f\"\\nüìã Encontradas {len(metrics_files)} ejecuciones anteriores:\\n\")\n",
    "    \n",
    "    execution_history = []\n",
    "    for json_file in metrics_files[-10:]:  # √öltimas 10\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        timestamp = data.get('timestamp', 'Unknown')\n",
    "        phases_success = sum(1 for p in data.get('phases', {}).values() if isinstance(p, dict) and p.get('status') == 'success')\n",
    "        phases_total = len(data.get('phases', {}))\n",
    "        \n",
    "        execution_history.append({\n",
    "            'Timestamp': timestamp,\n",
    "            'Fases OK': f\"{phases_success}/{phases_total}\",\n",
    "            'Archivo': Path(json_file).name\n",
    "        })\n",
    "    \n",
    "    if execution_history:\n",
    "        df_history = pd.DataFrame(execution_history)\n",
    "        print(df_history.to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nüìã No hay ejecuciones anteriores registradas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6332470",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Historial de Ejecuciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d1fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'session' in locals():\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"AN√ÅLISIS DE DATOS DE TELEMETR√çA\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    telemetry = session.telemetry_df\n",
    "    \n",
    "    # Estad√≠sticas generales\n",
    "    print(\"\\nüìä Velocidad (km/h):\")\n",
    "    print(f\"  Mean: {telemetry['speed_kmh'].mean():.2f}\")\n",
    "    print(f\"  Max: {telemetry['speed_kmh'].max():.2f}\")\n",
    "    print(f\"  Std: {telemetry['speed_kmh'].std():.2f}\")\n",
    "    \n",
    "    print(\"\\nüìä G-Force:\")\n",
    "    print(f\"  Mean: {telemetry['g_force'].mean():.2f}\")\n",
    "    print(f\"  Max: {telemetry['g_force'].max():.2f}\")\n",
    "    print(f\"  Std: {telemetry['g_force'].std():.2f}\")\n",
    "    \n",
    "    print(\"\\nüìä Lean Angle (grados):\")\n",
    "    print(f\"  Mean: {telemetry['lean_angle_deg'].mean():.2f}\")\n",
    "    print(f\"  Max: {telemetry['lean_angle_deg'].max():.2f}\")\n",
    "    print(f\"  Std: {telemetry['lean_angle_deg'].std():.2f}\")\n",
    "    \n",
    "    print(\"\\nüìä Fatigue:\")\n",
    "    print(f\"  Mean: {telemetry['fatigue'].mean():.3f}\")\n",
    "    print(f\"  Max: {telemetry['fatigue'].max():.3f}\")\n",
    "    \n",
    "    # Crear visualizaci√≥n de distribuciones\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    fig.suptitle('Distribuciones de Telemetr√≠a', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    axes[0, 0].hist(telemetry['speed_kmh'], bins=30, alpha=0.7, color='blue')\n",
    "    axes[0, 0].set_title('Velocidad (km/h)')\n",
    "    axes[0, 0].set_xlabel('km/h')\n",
    "    \n",
    "    axes[0, 1].hist(telemetry['g_force'], bins=30, alpha=0.7, color='green')\n",
    "    axes[0, 1].set_title('G-Force')\n",
    "    axes[0, 1].set_xlabel('G')\n",
    "    \n",
    "    axes[1, 0].hist(telemetry['lean_angle_deg'], bins=30, alpha=0.7, color='red')\n",
    "    axes[1, 0].set_title('Lean Angle (grados)')\n",
    "    axes[1, 0].set_xlabel('Grados')\n",
    "    \n",
    "    axes[1, 1].hist(telemetry['fatigue'], bins=30, alpha=0.7, color='orange')\n",
    "    axes[1, 1].set_title('Fatiga')\n",
    "    axes[1, 1].set_xlabel('Fatiga')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(LOGS_DIR / 'telemetry_distributions.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\n‚úÖ Gr√°fico de distribuciones guardado\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86b99cd",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Estad√≠sticas y An√°lisis de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca0ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FASE 6: PERSISTENCIA DE M√âTRICAS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Guardar m√©tricas en JSON\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# JSON completo\n",
    "json_file = METRICS_DIR / f'metrics_{timestamp}.json'\n",
    "with open(json_file, 'w') as f:\n",
    "    json.dump(metrics_log, f, indent=2)\n",
    "print(f\"‚úÖ M√©tricas JSON: {json_file}\")\n",
    "\n",
    "# CSV para an√°lisis\n",
    "csv_file = METRICS_DIR / f'metrics_summary_{timestamp}.csv'\n",
    "csv_rows = []\n",
    "for phase, data in metrics_log['phases'].items():\n",
    "    row = {'phase': phase, 'timestamp': metrics_log['timestamp']}\n",
    "    if isinstance(data, dict):\n",
    "        row.update(data)\n",
    "    csv_rows.append(row)\n",
    "\n",
    "df_metrics = pd.DataFrame(csv_rows)\n",
    "df_metrics.to_csv(csv_file, index=False)\n",
    "print(f\"‚úÖ M√©tricas CSV: {csv_file}\")\n",
    "\n",
    "# Resumen en texto\n",
    "txt_file = METRICS_DIR / f'summary_{timestamp}.txt'\n",
    "with open(txt_file, 'w') as f:\n",
    "    f.write(\"=\" * 70 + \"\\n\")\n",
    "    f.write(\"BIO-ADAPTIVE HAPTIC COACHING SYSTEM - EXECUTION SUMMARY\\n\")\n",
    "    f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "    f.write(f\"Timestamp: {metrics_log['timestamp']}\\n\")\n",
    "    f.write(f\"Project: {PROJECT_ROOT}\\n\\n\")\n",
    "    \n",
    "    for phase, data in metrics_log['phases'].items():\n",
    "        f.write(f\"\\n[{phase.upper()}]\\n\")\n",
    "        f.write(\"-\" * 70 + \"\\n\")\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                if isinstance(value, float):\n",
    "                    f.write(f\"  {key}: {value:.4f}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"  {key}: {value}\\n\")\n",
    "        else:\n",
    "            f.write(f\"  {data}\\n\")\n",
    "\n",
    "print(f\"‚úÖ Resumen TXT: {txt_file}\")\n",
    "\n",
    "# Mostrar resumen\n",
    "print(\"\\nüìä RESUMEN DE EJECUCI√ìN:\")\n",
    "print(df_metrics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee47412",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Persistir M√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e8ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASE 5: Visualizaci√≥n\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FASE 5: VISUALIZACI√ìN DE RESULTADOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    from visualize import create_visualization, visualize_training_metrics\n",
    "    \n",
    "    print(f\"\\n‚è≥ Generando visualizaciones...\")\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # Crear figura de visualizaci√≥n principal\n",
    "    if 'session' in locals() and 'eval_metrics' in locals():\n",
    "        fig = create_visualization(\n",
    "            session=session,\n",
    "            eval_metrics=eval_metrics,\n",
    "            save_path=LOGS_DIR / 'results_dashboard.png'\n",
    "        )\n",
    "        print(f\"‚úÖ Dashboard guardado: {LOGS_DIR / 'results_dashboard.png'}\")\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    metrics_log['phases']['visualization'] = {\n",
    "        'duration_seconds': elapsed,\n",
    "        'figures_generated': 1,\n",
    "        'status': 'success'\n",
    "    }\n",
    "    \n",
    "    plt.show()\n",
    "    print(f\"‚úÖ Visualizaciones completadas en {elapsed:.2f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error en visualizaci√≥n: {e}\")\n",
    "    metrics_log['phases']['visualization'] = {'status': 'warning', 'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bc56fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASE 4: Evaluaci√≥n\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FASE 4: EVALUACI√ìN DEL MODELO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    from evaluate import evaluate_trained_model\n",
    "    \n",
    "    print(f\"\\n‚è≥ Evaluando modelo entrenado...\")\n",
    "    \n",
    "    if 'model' in locals():\n",
    "        start = time.time()\n",
    "        eval_metrics = evaluate_trained_model(\n",
    "            model=model,\n",
    "            num_episodes=3,\n",
    "            render=False\n",
    "        )\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        metrics_log['phases']['evaluation'] = {\n",
    "            'duration_seconds': elapsed,\n",
    "            'episodes': 3,\n",
    "            'mean_episode_reward': eval_metrics.get('mean_reward', 0),\n",
    "            'mean_episode_length': eval_metrics.get('mean_length', 0),\n",
    "            'status': 'success'\n",
    "        }\n",
    "        \n",
    "        print(f\"‚úÖ Evaluaci√≥n completada en {elapsed:.2f}s\")\n",
    "        print(f\"   üìä Mean Reward: {eval_metrics.get('mean_reward', 0):.2f}\")\n",
    "        print(f\"   üìä Mean Length: {eval_metrics.get('mean_length', 0):.0f} steps\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Modelo no disponible, saltando evaluaci√≥n\")\n",
    "        metrics_log['phases']['evaluation'] = {'status': 'skipped'}\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error en evaluaci√≥n: {e}\")\n",
    "    metrics_log['phases']['evaluation'] = {'status': 'failed', 'error': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73c532",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Evaluaci√≥n y Visualizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c21593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASE 3: Entrenamiento\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FASE 3: ENTRENAMIENTO PPO\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    from train import create_training_environment, train_ppo_agent\n",
    "    \n",
    "    start = time.time()\n",
    "    print(f\"\\n‚è≥ Creando entorno de entrenamiento...\")\n",
    "    \n",
    "    train_env, _ = create_training_environment(n_laps=5, num_envs=1)\n",
    "    \n",
    "    print(f\"‚è≥ Entrenando modelo PPO (5000 timesteps)...\")\n",
    "    model, train_metrics = train_ppo_agent(\n",
    "        env=train_env,\n",
    "        total_timesteps=5000,\n",
    "        save_dir=MODELS_DIR\n",
    "    )\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    metrics_log['phases']['training'] = {\n",
    "        'duration_seconds': elapsed,\n",
    "        'timesteps': 5000,\n",
    "        'mean_reward': train_metrics.get('mean_reward', 0),\n",
    "        'max_reward': train_metrics.get('max_reward', 0),\n",
    "        'min_reward': train_metrics.get('min_reward', 0),\n",
    "        'status': 'success'\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Modelo entrenado en {elapsed:.2f}s\")\n",
    "    print(f\"   üìà Mean Reward: {train_metrics.get('mean_reward', 0):.2f}\")\n",
    "    print(f\"   üìà Max Reward: {train_metrics.get('max_reward', 0):.2f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error en entrenamiento: {e}\")\n",
    "    print(\"   (Continuando con evaluaci√≥n)\")\n",
    "    metrics_log['phases']['training'] = {'status': 'warning', 'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87242683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASE 2: Entorno\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FASE 2: ENTORNO GYMNASIUM\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    from environment import MotoBioEnv\n",
    "    \n",
    "    print(f\"\\n‚è≥ Creando entorno MotoBioEnv...\")\n",
    "    \n",
    "    env = MotoBioEnv(telemetry_df=session.telemetry_df)\n",
    "    obs, info = env.reset()\n",
    "    \n",
    "    # Probar algunos pasos\n",
    "    total_reward = 0\n",
    "    bio_gates = 0\n",
    "    for step in range(50):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if info.get('was_bio_gated', False):\n",
    "            bio_gates += 1\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    metrics_log['phases']['environment'] = {\n",
    "        'state_space': str(obs.shape),\n",
    "        'action_space': env.action_space.n,\n",
    "        'test_steps': step + 1,\n",
    "        'total_reward': total_reward,\n",
    "        'bio_gate_activations': bio_gates,\n",
    "        'status': 'success'\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Entorno creado\")\n",
    "    print(f\"   üìê Estado: {obs.shape}\")\n",
    "    print(f\"   üéÆ Acciones: {env.action_space.n}\")\n",
    "    print(f\"   üö® Bio-Gates: {bio_gates}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error en entorno: {e}\")\n",
    "    traceback.print_exc()\n",
    "    metrics_log['phases']['environment'] = {'status': 'failed', 'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fdf738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "metrics_log = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'phases': {}\n",
    "}\n",
    "\n",
    "# FASE 1: Generar datos\n",
    "print(\"=\" * 70)\n",
    "print(\"FASE 1: GENERACI√ìN DE DATOS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    from data_gen import SyntheticTelemetry\n",
    "    \n",
    "    start = time.time()\n",
    "    print(f\"\\n‚è≥ Generando 10 laps de datos sint√©ticos...\")\n",
    "    \n",
    "    gen = SyntheticTelemetry()\n",
    "    session = gen.generate_race_session(n_laps=10)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    metrics_log['phases']['data_generation'] = {\n",
    "        'duration_seconds': elapsed,\n",
    "        'laps': session.metadata['num_laps'],\n",
    "        'samples': len(session.telemetry_df),\n",
    "        'mean_speed_kmh': session.metadata['mean_speed_kmh'],\n",
    "        'max_speed_kmh': session.metadata['max_speed_kmh'],\n",
    "        'mean_hr_bpm': session.metadata['mean_hr_bpm'],\n",
    "        'status': 'success'\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Datos generados en {elapsed:.2f}s\")\n",
    "    print(f\"   üìä {len(session.telemetry_df)} muestras\")\n",
    "    print(f\"   üèÉ Velocidad media: {session.metadata['mean_speed_kmh']:.1f} km/h\")\n",
    "    print(f\"   üíì HR media: {session.metadata['mean_hr_bpm']:.1f} bpm\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error en generaci√≥n de datos: {e}\")\n",
    "    traceback.print_exc()\n",
    "    metrics_log['phases']['data_generation'] = {'status': 'failed', 'error': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80572ba6",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Generar Datos y Entrenar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37606254",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "try:\n",
    "    from config import (\n",
    "        SIM_CONFIG, REWARD_CONFIG, TRAIN_CONFIG, VIS_CONFIG, PATHS, get_config_summary\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Configuraci√≥n cargada desde config.py\")\n",
    "    \n",
    "    config_summary = get_config_summary()\n",
    "    config_df = pd.DataFrame(list(config_summary.items()), columns=['Par√°metro', 'Valor'])\n",
    "    print(\"\\nüìä Resumen de Configuraci√≥n:\")\n",
    "    print(config_df.to_string(index=False))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error cargando configuraci√≥n: {e}\")\n",
    "    SIM_CONFIG = None\n",
    "    REWARD_CONFIG = None\n",
    "    TRAIN_CONFIG = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98ba2a7",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Cargar Configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0198e988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_structure() -> Dict[str, bool]:\n",
    "    \"\"\"Validar estructura del proyecto\"\"\"\n",
    "    status = {}\n",
    "    \n",
    "    required_dirs = {\n",
    "        'src': SRC_DIR,\n",
    "        'models': MODELS_DIR,\n",
    "        'notebooks': NOTEBOOKS_DIR,\n",
    "        'scripts': SCRIPTS_DIR,\n",
    "        'logs': LOGS_DIR,\n",
    "        'data': DATA_DIR,\n",
    "    }\n",
    "    \n",
    "    required_files = {\n",
    "        'config.py': SRC_DIR / 'config.py',\n",
    "        'data_gen.py': SRC_DIR / 'data_gen.py',\n",
    "        'environment.py': SRC_DIR / 'environment.py',\n",
    "        'train.py': SRC_DIR / 'train.py',\n",
    "        'visualize.py': SRC_DIR / 'visualize.py',\n",
    "        'requirements.txt': PROJECT_ROOT / 'requirements.txt',\n",
    "    }\n",
    "    \n",
    "    print(\"üîç Validando directorios:\")\n",
    "    for name, path in required_dirs.items():\n",
    "        exists = path.exists()\n",
    "        status[f'dir_{name}'] = exists\n",
    "        symbol = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "        print(f\"  {symbol} {name}: {path}\")\n",
    "    \n",
    "    print(\"\\nüîç Validando archivos:\")\n",
    "    for name, path in required_files.items():\n",
    "        exists = path.exists()\n",
    "        status[f'file_{name}'] = exists\n",
    "        symbol = \"‚úÖ\" if exists else \"‚ùå\"\n",
    "        print(f\"  {symbol} {name}: {path}\")\n",
    "    \n",
    "    all_ok = all(status.values())\n",
    "    print(f\"\\n{'‚úÖ' if all_ok else '‚ö†Ô∏è'} Estructura: {'OK' if all_ok else 'INCOMPLETA'}\")\n",
    "    \n",
    "    return status\n",
    "\n",
    "structure_status = validate_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507dbc16",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Validar Estructura del Proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361eaa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import csv\n",
    "import logging\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Optional\n",
    "import traceback\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Data and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "# Setup paths\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if (PROJECT_ROOT / \"moto_bio_project\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT / \"moto_bio_project\"\n",
    "    \n",
    "SRC_DIR = PROJECT_ROOT / \"src\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "LOGS_DIR = PROJECT_ROOT / \"logs\"\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / \"notebooks\"\n",
    "SCRIPTS_DIR = PROJECT_ROOT / \"scripts\"\n",
    "METRICS_DIR = LOGS_DIR / \"metrics\"\n",
    "ARTIFACTS_DIR = LOGS_DIR / \"artifacts\"\n",
    "\n",
    "# Ensure directories exist\n",
    "for d in [MODELS_DIR, LOGS_DIR, DATA_DIR, METRICS_DIR, ARTIFACTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(LOGS_DIR / 'notebook_run.log')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Entorno configurado\")\n",
    "print(f\"üìÅ Ra√≠z del proyecto: {PROJECT_ROOT}\")\n",
    "print(f\"üìÅ Src: {SRC_DIR}\")\n",
    "print(f\"üìÅ Modelos: {MODELS_DIR}\")\n",
    "print(f\"üìÅ Logs: {LOGS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d326cd2a",
   "metadata": {},
   "source": [
    "# üèçÔ∏è Bio-Adaptive Haptic Coaching System\n",
    "## Notebook de An√°lisis y Orquestaci√≥n Completa\n",
    "\n",
    "**Prop√≥sito**: Ejecutar, visualizar y documentar el deployment completo del sistema\n",
    "\n",
    "- ‚úÖ Validar estructura del proyecto\n",
    "- ‚úÖ Preparar dependencias\n",
    "- ‚úÖ Ejecutar training y evaluation\n",
    "- ‚úÖ Guardar m√©tricas\n",
    "- ‚úÖ Visualizar resultados\n",
    "- ‚úÖ Generar reportes\n",
    "\n",
    "**Fecha**: 2025-01-17 | **Versi√≥n**: 1.0"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
